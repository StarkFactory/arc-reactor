package com.arc.reactor.promptlab.analysis

import com.arc.reactor.config.ChatModelProvider
import com.arc.reactor.prompt.PromptTemplateStore
import com.arc.reactor.prompt.PromptVersion
import com.arc.reactor.prompt.VersionStatus
import com.arc.reactor.promptlab.model.FeedbackAnalysis
import com.arc.reactor.promptlab.model.PromptWeakness
import io.kotest.assertions.throwables.shouldThrow
import io.kotest.matchers.collections.shouldBeEmpty
import io.kotest.matchers.collections.shouldHaveSize
import io.kotest.matchers.shouldBe
import io.kotest.matchers.string.shouldContain
import io.mockk.every
import io.mockk.mockk
import io.mockk.verify
import kotlinx.coroutines.test.runTest
import org.junit.jupiter.api.BeforeEach
import org.junit.jupiter.api.Nested
import org.junit.jupiter.api.Test
import org.springframework.ai.chat.client.ChatClient

class PromptCandidateGeneratorTest {

    private val chatModelProvider: ChatModelProvider = mockk()
    private val promptTemplateStore: PromptTemplateStore = mockk()
    private val chatClient: ChatClient = mockk()
    private val requestSpec: ChatClient.ChatClientRequestSpec = mockk(relaxed = true)
    private val callResponseSpec: ChatClient.CallResponseSpec = mockk()

    private lateinit var generator: PromptCandidateGenerator

    private val activeVersion = PromptVersion(
        id = "v-1",
        templateId = "template-1",
        version = 1,
        content = "You are a helpful customer support agent.",
        status = VersionStatus.ACTIVE
    )

    private val analysis = FeedbackAnalysis(
        totalFeedback = 10,
        negativeCount = 5,
        weaknesses = listOf(
            PromptWeakness(
                category = "short_answer",
                description = "Responses too brief",
                frequency = 3,
                exampleQueries = listOf("How to reset?")
            ),
            PromptWeakness(
                category = "missing_sources",
                description = "No references",
                frequency = 2,
                exampleQueries = listOf("Where are docs?")
            )
        ),
        sampleQueries = emptyList()
    )

    @BeforeEach
    fun setup() {
        generator = PromptCandidateGenerator(chatModelProvider, promptTemplateStore)
        every { chatModelProvider.getChatClient(any()) } returns chatClient
        every { chatClient.prompt() } returns requestSpec
        every { requestSpec.user(any<String>()) } returns requestSpec
        every { requestSpec.call() } returns callResponseSpec
    }

    @Nested
    inner class SuccessfulGeneration {

        @Test
        fun `should generate candidate versions and return their IDs`() = runTest {
            every { promptTemplateStore.getActiveVersion("template-1") } returns activeVersion
            val llmResponse = """["Improved prompt 1", "Improved prompt 2", "Improved prompt 3"]"""
            every { callResponseSpec.content() } returns llmResponse

            every { promptTemplateStore.createVersion("template-1", "Improved prompt 1", any()) } returns
                PromptVersion(id = "v-2", templateId = "template-1", version = 2, content = "Improved prompt 1")
            every { promptTemplateStore.createVersion("template-1", "Improved prompt 2", any()) } returns
                PromptVersion(id = "v-3", templateId = "template-1", version = 3, content = "Improved prompt 2")
            every { promptTemplateStore.createVersion("template-1", "Improved prompt 3", any()) } returns
                PromptVersion(id = "v-4", templateId = "template-1", version = 4, content = "Improved prompt 3")

            val result = generator.generate("template-1", analysis)

            result shouldHaveSize 3 // "Should return 3 version IDs"
            result shouldBe listOf("v-2", "v-3", "v-4") // "Version IDs should match"
        }

        @Test
        fun `should pass changelog with weakness categories`() = runTest {
            every { promptTemplateStore.getActiveVersion("template-1") } returns activeVersion
            every { callResponseSpec.content() } returns """["Improved prompt"]"""

            var capturedChangeLog = ""
            every { promptTemplateStore.createVersion("template-1", "Improved prompt", any()) } answers {
                capturedChangeLog = thirdArg()
                PromptVersion(id = "v-2", templateId = "template-1", version = 2, content = "Improved prompt")
            }

            generator.generate("template-1", analysis, candidateCount = 1)

            capturedChangeLog shouldContain "short_answer" // "Changelog should mention weakness category"
            capturedChangeLog shouldContain "missing_sources" // "Changelog should mention all categories"
            capturedChangeLog shouldContain "Auto-generated by Prompt Lab" // "Changelog should have prefix"
        }

        @Test
        fun `should respect candidateCount parameter`() = runTest {
            every { promptTemplateStore.getActiveVersion("template-1") } returns activeVersion
            every { callResponseSpec.content() } returns """["P1", "P2"]"""
            every { promptTemplateStore.createVersion("template-1", "P1", any()) } returns
                PromptVersion(id = "v-2", templateId = "template-1", version = 2, content = "P1")
            every { promptTemplateStore.createVersion("template-1", "P2", any()) } returns
                PromptVersion(id = "v-3", templateId = "template-1", version = 3, content = "P2")

            val result = generator.generate("template-1", analysis, candidateCount = 2)

            result shouldHaveSize 2 // "Should generate requested number of candidates"
        }

        @Test
        fun `should handle LLM response wrapped in code fences`() = runTest {
            every { promptTemplateStore.getActiveVersion("template-1") } returns activeVersion
            val wrappedResponse = """
                ```json
                ["Improved prompt"]
                ```
            """.trimIndent()
            every { callResponseSpec.content() } returns wrappedResponse
            every { promptTemplateStore.createVersion("template-1", "Improved prompt", any()) } returns
                PromptVersion(id = "v-2", templateId = "template-1", version = 2, content = "Improved prompt")

            val result = generator.generate("template-1", analysis, candidateCount = 1)

            result shouldHaveSize 1 // "Should parse JSON inside code fences"
        }
    }

    @Nested
    inner class NoActiveVersion {

        @Test
        fun `should throw IllegalStateException when no active version exists`() = runTest {
            every { promptTemplateStore.getActiveVersion("template-1") } returns null

            val exception = shouldThrow<IllegalStateException> {
                generator.generate("template-1", analysis)
            }
            exception.message shouldContain "template-1" // "Error should reference template ID"
        }

        @Test
        fun `should not call LLM when no active version`() = runTest {
            every { promptTemplateStore.getActiveVersion("template-1") } returns null

            try {
                generator.generate("template-1", analysis)
            } catch (_: IllegalStateException) {
                // expected
            }

            verify(exactly = 0) { chatClient.prompt() }
        }
    }

    @Nested
    inner class LlmErrorHandling {

        @Test
        fun `should return empty list when LLM returns invalid JSON`() = runTest {
            every { promptTemplateStore.getActiveVersion("template-1") } returns activeVersion
            every { callResponseSpec.content() } returns "not json"

            val result = generator.generate("template-1", analysis)

            result.shouldBeEmpty() // "Invalid JSON should produce empty result"
        }

        @Test
        fun `should return empty list when LLM returns empty response`() = runTest {
            every { promptTemplateStore.getActiveVersion("template-1") } returns activeVersion
            every { callResponseSpec.content() } returns ""

            val result = generator.generate("template-1", analysis)

            result.shouldBeEmpty() // "Empty response should produce empty result"
        }

        @Test
        fun `should return empty list when LLM returns null content`() = runTest {
            every { promptTemplateStore.getActiveVersion("template-1") } returns activeVersion
            every { callResponseSpec.content() } returns null

            val result = generator.generate("template-1", analysis)

            result.shouldBeEmpty() // "Null content should produce empty result"
        }

        @Test
        fun `should return empty list when LLM call throws exception`() = runTest {
            every { promptTemplateStore.getActiveVersion("template-1") } returns activeVersion
            every { callResponseSpec.content() } throws RuntimeException("API error")

            val result = generator.generate("template-1", analysis)

            result.shouldBeEmpty() // "Exception should produce empty result"
        }

        @Test
        fun `should return empty list when LLM returns non-array JSON`() = runTest {
            every { promptTemplateStore.getActiveVersion("template-1") } returns activeVersion
            every { callResponseSpec.content() } returns """{"key": "value"}"""

            val result = generator.generate("template-1", analysis)

            result.shouldBeEmpty() // "Non-array JSON should produce empty result"
        }

        @Test
        fun `should skip blank candidate strings`() = runTest {
            every { promptTemplateStore.getActiveVersion("template-1") } returns activeVersion
            every { callResponseSpec.content() } returns """["Valid prompt", "", "  "]"""
            every { promptTemplateStore.createVersion("template-1", "Valid prompt", any()) } returns
                PromptVersion(id = "v-2", templateId = "template-1", version = 2, content = "Valid prompt")

            val result = generator.generate("template-1", analysis, candidateCount = 3)

            result shouldHaveSize 1 // "Should skip blank candidates"
        }
    }

    @Nested
    inner class StoreInteraction {

        @Test
        fun `should handle createVersion returning null`() = runTest {
            every { promptTemplateStore.getActiveVersion("template-1") } returns activeVersion
            every { callResponseSpec.content() } returns """["Prompt 1", "Prompt 2"]"""
            every { promptTemplateStore.createVersion("template-1", "Prompt 1", any()) } returns null
            every { promptTemplateStore.createVersion("template-1", "Prompt 2", any()) } returns
                PromptVersion(id = "v-3", templateId = "template-1", version = 3, content = "Prompt 2")

            val result = generator.generate("template-1", analysis, candidateCount = 2)

            result shouldHaveSize 1 // "Should skip null versions from store"
            result[0] shouldBe "v-3" // "Should contain the successfully created version"
        }
    }
}
